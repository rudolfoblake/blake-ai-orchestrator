# âšª **Blake AI Orchestrator â€“ Backend**

FastAPI backend orchestrating multiple AIs (**OpenAI**, **Claude**, **DeepSeek**, **Gemini**), running parallel calls and aggregating into a final inference.

---

## ğŸ”— See Also
- [Project overview](../README.md)
- [Frontend docs](../frontend/README.md)
- [Monitoring docs](../monitoring/README.md)
- [Database docs](../database/README.md)

---

## âš™ï¸ Requirements
- Python 3.11
- Optional: Redis and PostgreSQL for cache and logs

---

## ğŸ§  Setup
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r backend/requirements.txt
```

---

## ğŸš€ Run
```bash
uvicorn backend.orchestrator.main:app --reload
```
- API â†’ [http://localhost:8000](http://localhost:8000)  
- Health â†’ `GET /health`  
- Inference â†’ `POST /infer` â†’ `{ "prompt": "..." }`  
- Metrics â†’ `GET /metrics`  
- Docs â†’ [Swagger](http://localhost:8000/docs) | [ReDoc](http://localhost:8000/redoc)

---

## ğŸŒ Environment Variables
See `.env.example` at the project root. Highlights:

- Keys â†’ `OPENAI_API_KEY`, `CLAUDE_API_KEY`, `DEEPSEEK_API_KEY`, `GEMINI_API_KEY`  
- Enabled providers â†’ `ENABLED_PROVIDERS=openai,claude,deepseek,gemini`  
- Personas â†’ `PERSONA_DEFAULT`, `PERSONA_OPENAI`, `PERSONA_CLAUDE`, etc.  
- Models â†’ `OPENAI_MODEL`, `CLAUDE_MODEL`, `DEEPSEEK_MODEL`, `GEMINI_MODEL`  
- Params â†’ `TEMPERATURE`, `MAX_TOKENS`, weights `INFER_WEIGHT_*`  
- Infra â†’ `DATABASE_URL`, `REDIS_URL`  

---

## ğŸ§© Flow
1. `POST /infer` receives `{ prompt }`
2. `services/inferencer.py` fires async calls to enabled providers
3. `services/analyzer.py` computes similarity/confidence/context (SentenceTransformer/fallback)
4. Returns `{ final_answer, confidence, sources }`

---

## ğŸª¶ Decoupling
- Fully independent backend; serves any frontend via REST  
- CORS open (`*`) by default â†’ adjust as needed  
- Redis/Postgres optional â†’ API runs standalone  

---

## ğŸ§± Structure
```
orchestrator/
  main.py
  router_infer.py
  services/
    openai_client.py
    claude_client.py
    deepseek_client.py
    gemini_client.py
    analyzer.py
    inferencer.py
  utils/
    logging.py
    cache.py
    database.py
  __init__.py
```

---

## ğŸ§ª Quick Test
```bash
curl -X POST http://localhost:8000/infer   -H "Content-Type: application/json"   -d '{"prompt":"Explain AI in the creative economy"}'
```

---

## ğŸ‡§ğŸ‡· **PT-BR**

Backend **FastAPI** que orquestra mÃºltiplas IAs (**OpenAI**, **Claude**, **DeepSeek**, **Gemini**), realiza chamadas paralelas e combina respostas em uma inferÃªncia final.

---

### ğŸ”— Veja TambÃ©m
- [VisÃ£o geral do projeto](../README.md)
- [Docs do Frontend](../frontend/README.md)
- [Docs de Monitoring](../monitoring/README.md)
- [Docs de Database](../database/README.md)

---

### âš™ï¸ Requisitos
- Python 3.11  
- (Opcional) Redis e PostgreSQL para cache e logs

---

### ğŸ§  InstalaÃ§Ã£o
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r backend/requirements.txt
```

---

### ğŸš€ ExecuÃ§Ã£o
```bash
uvicorn backend.orchestrator.main:app --reload
```
- API â†’ [http://localhost:8000](http://localhost:8000)  
- SaÃºde â†’ `GET /health`  
- InferÃªncia â†’ `POST /infer` â†’ `{ "prompt": "..." }`  
- MÃ©tricas Prometheus â†’ `GET /metrics`  
- Docs â†’ [Swagger](http://localhost:8000/docs) | [ReDoc](http://localhost:8000/redoc)

---

### ğŸŒ VariÃ¡veis de Ambiente
Veja `.env.example` na raiz do projeto. Principais:

- Chaves â†’ `OPENAI_API_KEY`, `CLAUDE_API_KEY`, `DEEPSEEK_API_KEY`, `GEMINI_API_KEY`  
- Provedores habilitados â†’ `ENABLED_PROVIDERS=openai,claude,deepseek,gemini`  
- Personas â†’ `PERSONA_DEFAULT`, `PERSONA_OPENAI`, `PERSONA_CLAUDE`, etc.  
- Modelos â†’ `OPENAI_MODEL`, `CLAUDE_MODEL`, `DEEPSEEK_MODEL`, `GEMINI_MODEL`  
- ParÃ¢metros â†’ `TEMPERATURE`, `MAX_TOKENS`, pesos `INFER_WEIGHT_*`  
- Infra â†’ `DATABASE_URL`, `REDIS_URL`  

---

### ğŸ” Fluxo
1. `POST /infer` recebe `{ prompt }`
2. `services/inferencer.py` dispara chamadas assÃ­ncronas aos provedores habilitados
3. `services/analyzer.py` calcula similaridade/confianÃ§a/contexto (SentenceTransformer/fallback)
4. Retorna `{ final_answer, confidence, sources }`

---

### ğŸª¶ Desacoplamento
- Backend totalmente independente; expÃµe REST para qualquer frontend  
- CORS liberado (`*`) por padrÃ£o; ajuste conforme necessidade  
- Redis e Postgres opcionais; a API funciona sem eles  

---

### ğŸ§± Estrutura
```
orchestrator/
  main.py
  router_infer.py
  services/
    openai_client.py
    claude_client.py
    deepseek_client.py
    gemini_client.py
    analyzer.py
    inferencer.py
  utils/
    logging.py
    cache.py
    database.py
  __init__.py
```

---

### ğŸ§ª Teste RÃ¡pido
```bash
curl -X POST http://localhost:8000/infer   -H "Content-Type: application/json"   -d '{"prompt":"Explique IA na economia criativa"}'
```
